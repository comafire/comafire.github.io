{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark DataFrame 에서 복잡한 데이터 포맷 다루기 \n",
    "\n",
    "참조\n",
    "* [Working with Complex Data Formats with Structured Streaming in Apache Spark 2.1](https://databricks.com/blog/2017/02/23/working-complex-data-formats-structured-streaming-apache-spark-2-1.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/spark-2.4.1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Python Version Mismatch Error 일 경우, os.environ 으로 직접 설정 후 실행\n",
    "# Exception: Python in worker has different version 2.7 than that in driver 3.5, \n",
    "# PySpark cannot run with different minor versions.Please check environment variables \n",
    "# PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n",
    "\n",
    "# os.environ[\"PYSPARK_PYTHON\"] = \"/usr/bin/python3\"\n",
    "# os.environ[\"PYSPARK_DRIVER_PYTHON\"] = \"/usr/bin/python3\"\n",
    "\n",
    "spark_home = os.environ.get('SPARK_HOME', None)\n",
    "print(spark_home)\n",
    "\n",
    "spark = SparkSession.builder.master(\"local[*]\").appName(\"spark\")\n",
    "spark = spark.config(\"spark.driver.memory\", \"4g\")\n",
    "spark = spark.config(\"spark.executor.memory\", \"4g\")\n",
    "spark = spark.config(\"spark.python.worker.memory\", \"4g\")\n",
    "spark = spark.getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrame Complex Data Type\n",
    "\n",
    "웹에서 시작된 JSON 포맷은 현재 인기있는 Raw Data Format 으로 자리잡았습니다. 따라서, Spark을 통해서 데이터 엔지니어링을 할때 많이 다루게 되는 포맷이 JSON 포맷입니다. 여기서는 JSON 에서 대표적으로 사용되는 Complex Data Format을 다루는 방법에 대해서 알아보겠습니다.\n",
    "\n",
    "![complex data format](https://databricks.com/wp-content/uploads/2017/02/various-data-types.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- a_struct: struct (nullable = true)\n",
      " |    |-- field_1: long (nullable = true)\n",
      " |    |-- field_2: string (nullable = true)\n",
      " |    |-- field_3: struct (nullable = true)\n",
      " |    |    |-- key_1: long (nullable = true)\n",
      " |    |    |-- key_2: long (nullable = true)\n",
      " |    |    |-- key_3: long (nullable = true)\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# import pyspark class\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql import functions as f\n",
    "from pyspark.sql import types as t\n",
    "\n",
    "json_str_struct = \"\"\"\n",
    "{\n",
    "  \"a_struct\": {\n",
    "     \"field_1\": 1,\n",
    "     \"field_2\": \"b\",\n",
    "     \"field_3\": {\n",
    "         \"key_1\": 1,\n",
    "         \"key_2\": 2,\n",
    "         \"key_3\": 3\n",
    "     }\n",
    "  }\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "rdd_struct = sc.parallelize([json_str_struct])\n",
    "df_struct = spark.read.json(rdd_struct)\n",
    "print(\"{}\".format(df_struct.printSchema()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- a_map: map (nullable = false)\n",
      " |    |-- key: string\n",
      " |    |-- value: long (valueContainsNull = true)\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# json string 은 기본적으로 struct type 으로 parsing 되기 때문에 map type 으로 받으려면 schema 를 지정해줘야 합니다.\n",
    "schema_map = t.StructType([\n",
    "    t.StructField(\"a_map\", t.MapType(t.StringType(), t.LongType()), False)\n",
    "])\n",
    "\n",
    "json_str_map = \"\"\"\n",
    "{\n",
    "  \"a_map\": {\n",
    "     \"key_1\": 1,\n",
    "     \"key_2\": 2,\n",
    "     \"key_3\": 3\n",
    "  }\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "rdd_map = sc.parallelize([json_str_map])\n",
    "df_map = spark.read.json(rdd_map, schema_map)\n",
    "print(\"{}\".format(df_map.printSchema()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- an_array: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "json_str_array = \"\"\"\n",
    "{\n",
    "  \"an_array\": [\n",
    "      \"Allice\", \n",
    "      \"Bob\", \n",
    "      \"Chris\"\n",
    "  ]\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "rdd_array = sc.parallelize([json_str_array])\n",
    "df_array = spark.read.json(rdd_array)\n",
    "print(\"{}\".format(df_array.printSchema()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seleting from nested columns \n",
    "struct 와 map 의 nested column 접근시에는 '.' 문자를 사용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- b: long (nullable = true)\n",
      "\n",
      "+---+\n",
      "|  b|\n",
      "+---+\n",
      "|  1|\n",
      "+---+\n",
      "\n",
      "root\n",
      " |-- b: long (nullable = true)\n",
      "\n",
      "+---+\n",
      "|  b|\n",
      "+---+\n",
      "|  1|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "json_str = \"\"\"\n",
    "{\n",
    "  \"a\": {\n",
    "     \"b\": 1\n",
    "  }\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "rdd = sc.parallelize([json_str])\n",
    "df = spark.read.json(rdd)\n",
    "df.createOrReplaceTempView(\"view\")\n",
    "\n",
    "df_res = df.select(\"a.b\")\n",
    "df_res.printSchema()\n",
    "df_res.show()\n",
    "\n",
    "df_res = spark.sql(\"select a.b from view\")\n",
    "df_res.printSchema()\n",
    "df_res.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flattening structs\n",
    "\n",
    "struct 에서 모든 서브 필드를 가져오고 싶으면 '*' 문자를 사용하면 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- b: long (nullable = true)\n",
      " |-- c: long (nullable = true)\n",
      "\n",
      "+---+---+\n",
      "|  b|  c|\n",
      "+---+---+\n",
      "|  1|  2|\n",
      "+---+---+\n",
      "\n",
      "root\n",
      " |-- b: long (nullable = true)\n",
      " |-- c: long (nullable = true)\n",
      "\n",
      "+---+---+\n",
      "|  b|  c|\n",
      "+---+---+\n",
      "|  1|  2|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "json_str = \"\"\"\n",
    "{\n",
    "  \"a\": {\n",
    "     \"b\": 1,\n",
    "     \"c\": 2\n",
    "  }\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "rdd = sc.parallelize([json_str])\n",
    "df = spark.read.json(rdd)\n",
    "df.createOrReplaceTempView(\"view\")\n",
    "\n",
    "df_res = df.select(\"a.*\")\n",
    "df_res.printSchema()\n",
    "df_res.show()\n",
    "\n",
    "df_res = spark.sql(\"select a.* from view\")\n",
    "df_res.printSchema()\n",
    "df_res.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nesting Columns\n",
    "\n",
    "struct 함수를 이용하면 새로운 struct 생성이 가능합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- x: struct (nullable = false)\n",
      " |    |-- y: long (nullable = true)\n",
      "\n",
      "+---+\n",
      "|  x|\n",
      "+---+\n",
      "|[1]|\n",
      "+---+\n",
      "\n",
      "root\n",
      " |-- x: struct (nullable = false)\n",
      " |    |-- y: long (nullable = true)\n",
      "\n",
      "+---+\n",
      "|  x|\n",
      "+---+\n",
      "|[1]|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "json_str = \"\"\"\n",
    "{\n",
    "  \"a\": 1,\n",
    "  \"b\": 2,\n",
    "  \"c\": 3\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "rdd = sc.parallelize([json_str])\n",
    "df = spark.read.json(rdd)\n",
    "df.createOrReplaceTempView(\"view\")\n",
    "\n",
    "df_res = df.select(f.struct(f.col(\"a\").alias(\"y\")).alias(\"x\"))\n",
    "df_res.printSchema()\n",
    "df_res.show()\n",
    "\n",
    "df_res = spark.sql('select named_struct(\"y\", a) as x from view')\n",
    "df_res.printSchema()\n",
    "df_res.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nesting all columns\n",
    "\n",
    "모든 서브필드를 struct로 만들고 싶으면 '*' 문자를 사용하면 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- x: struct (nullable = false)\n",
      " |    |-- a: long (nullable = true)\n",
      " |    |-- b: long (nullable = true)\n",
      "\n",
      "+------+\n",
      "|     x|\n",
      "+------+\n",
      "|[1, 2]|\n",
      "+------+\n",
      "\n",
      "root\n",
      " |-- x: struct (nullable = false)\n",
      " |    |-- a: long (nullable = true)\n",
      " |    |-- b: long (nullable = true)\n",
      "\n",
      "+------+\n",
      "|     x|\n",
      "+------+\n",
      "|[1, 2]|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "json_str = \"\"\"\n",
    "{\n",
    "  \"a\": 1,\n",
    "  \"b\": 2\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "rdd = sc.parallelize([json_str])\n",
    "df = spark.read.json(rdd)\n",
    "df.createOrReplaceTempView(\"view\")\n",
    "\n",
    "df_res = df.select(f.struct(\"*\").alias(\"x\"))\n",
    "df_res.printSchema()\n",
    "df_res.show()\n",
    "\n",
    "df_res = spark.sql('select struct(*) as x from view')\n",
    "df_res.printSchema()\n",
    "df_res.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting a single array or map element\n",
    "\n",
    "getItem 함수를 이용하면 array, map 에서 element 를 가져올 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- x: long (nullable = true)\n",
      "\n",
      "+---+\n",
      "|  x|\n",
      "+---+\n",
      "|  1|\n",
      "+---+\n",
      "\n",
      "root\n",
      " |-- x: long (nullable = true)\n",
      "\n",
      "+---+\n",
      "|  x|\n",
      "+---+\n",
      "|  1|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "json_str = \"\"\"\n",
    "{\n",
    "  \"a\": [1, 2]\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "rdd = sc.parallelize([json_str])\n",
    "df = spark.read.json(rdd)\n",
    "df.createOrReplaceTempView(\"view\")\n",
    "\n",
    "df_res = df.select(f.col(\"a\").getItem(0).alias(\"x\"))\n",
    "df_res.printSchema()\n",
    "df_res.show()\n",
    "\n",
    "df_res = spark.sql('select a[0] as x from view')\n",
    "df_res.printSchema()\n",
    "df_res.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- x: long (nullable = true)\n",
      "\n",
      "+---+\n",
      "|  x|\n",
      "+---+\n",
      "|  1|\n",
      "+---+\n",
      "\n",
      "root\n",
      " |-- x: long (nullable = true)\n",
      "\n",
      "+---+\n",
      "|  x|\n",
      "+---+\n",
      "|  1|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "json_str = \"\"\"\n",
    "{\n",
    "  \"a\": {\n",
    "    \"b\": 1\n",
    "  }\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "rdd = sc.parallelize([json_str])\n",
    "df = spark.read.json(rdd)\n",
    "df.createOrReplaceTempView(\"view\")\n",
    "\n",
    "df_res = df.select(f.col(\"a\").getItem(\"b\").alias(\"x\"))\n",
    "df_res.printSchema()\n",
    "df_res.show()\n",
    "\n",
    "df_res = spark.sql(\"select a['b'] as x from view\")\n",
    "df_res.printSchema()\n",
    "df_res.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a row for each array or map element\n",
    "\n",
    "explode함수를 이용하여 array 나 map 의 key-value 를 위한 새로운 row를 생성할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- x: long (nullable = true)\n",
      "\n",
      "+---+\n",
      "|  x|\n",
      "+---+\n",
      "|  1|\n",
      "|  2|\n",
      "+---+\n",
      "\n",
      "root\n",
      " |-- x: long (nullable = true)\n",
      "\n",
      "+---+\n",
      "|  x|\n",
      "+---+\n",
      "|  1|\n",
      "|  2|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "json_str = \"\"\"\n",
    "{\n",
    "  \"a\": [1, 2]\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "rdd = sc.parallelize([json_str])\n",
    "df = spark.read.json(rdd)\n",
    "df.createOrReplaceTempView(\"view\")\n",
    "\n",
    "df_res = df.select(f.explode(\"a\").alias(\"x\"))\n",
    "df_res.printSchema()\n",
    "df_res.show()\n",
    "\n",
    "df_res = spark.sql(\"select explode(a) as x from view\")\n",
    "df_res.printSchema()\n",
    "df_res.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- x: string (nullable = false)\n",
      " |-- y: long (nullable = true)\n",
      "\n",
      "+---+---+\n",
      "|  x|  y|\n",
      "+---+---+\n",
      "|  b|  1|\n",
      "|  c|  2|\n",
      "+---+---+\n",
      "\n",
      "root\n",
      " |-- x: string (nullable = false)\n",
      " |-- y: long (nullable = true)\n",
      "\n",
      "+---+---+\n",
      "|  x|  y|\n",
      "+---+---+\n",
      "|  b|  1|\n",
      "|  c|  2|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "json_str = \"\"\"\n",
    "{\n",
    "  \"a\": {\n",
    "    \"b\": 1,\n",
    "    \"c\": 2\n",
    "  }\n",
    "}\n",
    "\"\"\"\n",
    "schema = t.StructType([\n",
    "    t.StructField(\"a\", t.MapType(t.StringType(), t.LongType()), False)\n",
    "])\n",
    "\n",
    "rdd = sc.parallelize([json_str])\n",
    "df = spark.read.json(rdd, schema)\n",
    "df.createOrReplaceTempView(\"view\")\n",
    "\n",
    "df_res = df.select(f.explode(\"a\").alias(\"x\", \"y\"))\n",
    "df_res.printSchema()\n",
    "df_res.show()\n",
    "\n",
    "df_res = spark.sql(\"select explode(a) as (x, y) from view\")\n",
    "df_res.printSchema()\n",
    "df_res.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collecting multiple rows into an array\n",
    "\n",
    "collect_list, collect_set 함수는 array 에 여러 row (또는 연산된 row)를 넣을 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- x: array (nullable = true)\n",
      " |    |-- element: long (containsNull = true)\n",
      "\n",
      "+------+\n",
      "|     x|\n",
      "+------+\n",
      "|[1, 2]|\n",
      "+------+\n",
      "\n",
      "root\n",
      " |-- x: array (nullable = true)\n",
      " |    |-- element: long (containsNull = true)\n",
      "\n",
      "+------+\n",
      "|     x|\n",
      "+------+\n",
      "|[1, 2]|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "json_str = \"\"\"\n",
    "[{ \"x\": 1 }, { \"x\": 2 }]\n",
    "\"\"\"\n",
    "\n",
    "rdd = sc.parallelize([json_str])\n",
    "df = spark.read.json(rdd)\n",
    "df.createOrReplaceTempView(\"view\")\n",
    "\n",
    "df_res = df.select(f.collect_list(\"x\").alias(\"x\"))\n",
    "df_res.printSchema()\n",
    "df_res.show()\n",
    "\n",
    "df_res = spark.sql(\"select collect_list(x) as x from view\")\n",
    "df_res.printSchema()\n",
    "df_res.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- y: string (nullable = true)\n",
      " |-- x: array (nullable = true)\n",
      " |    |-- element: long (containsNull = true)\n",
      "\n",
      "+---+---+\n",
      "|  y|  x|\n",
      "+---+---+\n",
      "|  b|[2]|\n",
      "|  a|[1]|\n",
      "+---+---+\n",
      "\n",
      "root\n",
      " |-- y: string (nullable = true)\n",
      " |-- x: array (nullable = true)\n",
      " |    |-- element: long (containsNull = true)\n",
      "\n",
      "+---+---+\n",
      "|  y|  x|\n",
      "+---+---+\n",
      "|  b|[2]|\n",
      "|  a|[1]|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "json_str = \"\"\"\n",
    "[{ \"x\": 1, \"y\": \"a\" }, { \"x\": 2, \"y\": \"b\" }]\n",
    "\"\"\"\n",
    "\n",
    "rdd = sc.parallelize([json_str])\n",
    "df = spark.read.json(rdd)\n",
    "df.createOrReplaceTempView(\"view\")\n",
    "\n",
    "df_res = df.groupBy(\"y\").agg(f.collect_list(\"x\").alias(\"x\"))\n",
    "df_res.printSchema()\n",
    "df_res.show()\n",
    "\n",
    "df_res = spark.sql(\"select y, collect_list(x) as x from view group by y\")\n",
    "df_res.printSchema()\n",
    "df_res.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting one field from each item in an array\n",
    "\n",
    "array 에서 '.' 문자를 사용하면 값을 array 형태로 돌려주게 됩니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- b: array (nullable = true)\n",
      " |    |-- element: long (containsNull = true)\n",
      "\n",
      "+------+\n",
      "|     b|\n",
      "+------+\n",
      "|[1, 2]|\n",
      "+------+\n",
      "\n",
      "root\n",
      " |-- b: array (nullable = true)\n",
      " |    |-- element: long (containsNull = true)\n",
      "\n",
      "+------+\n",
      "|     b|\n",
      "+------+\n",
      "|[1, 2]|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "json_str = \"\"\"\n",
    "{\n",
    "  \"a\": [\n",
    "    {\"b\": 1},\n",
    "    {\"b\": 2}\n",
    "  ]\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "rdd = sc.parallelize([json_str])\n",
    "df = spark.read.json(rdd)\n",
    "df.createOrReplaceTempView(\"view\")\n",
    "\n",
    "df_res = df.select(f.col(\"a.b\"))\n",
    "df_res.printSchema()\n",
    "df_res.show()\n",
    "\n",
    "df_res = spark.sql(\"select a.b from view\")\n",
    "df_res.printSchema()\n",
    "df_res.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Power of to_json() and from_json()\n",
    "\n",
    "DataFrame 내에 컬럼에 대해 to_json 및 from_json 함수로 JSON 포맷 데이터에 대해 encode/decode 를 편리하게 할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode a struct as json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- a: string (nullable = true)\n",
      "\n",
      "+-------+\n",
      "|      a|\n",
      "+-------+\n",
      "|{\"b\":1}|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "json_str = \"\"\"\n",
    "{\n",
    "  \"a\": {\n",
    "    \"b\": 1\n",
    "  }\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "rdd = sc.parallelize([json_str])\n",
    "df = spark.read.json(rdd)\n",
    "\n",
    "# encdoe json_str\n",
    "df_res = df.select(f.to_json(\"a\").alias(\"a\"))\n",
    "df_res.printSchema()\n",
    "df_res.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decode json column as a struct "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|        a|\n",
      "+---------+\n",
      "|{\"b\":1}\"}|\n",
      "+---------+\n",
      "\n",
      "root\n",
      " |-- c: struct (nullable = true)\n",
      " |    |-- b: integer (nullable = true)\n",
      "\n",
      "+---+\n",
      "|  c|\n",
      "+---+\n",
      "|[1]|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "json_str = \"\"\"{\\\"b\\\":1}\"}\"\"\"\n",
    "\n",
    "rdd = sc.parallelize([json_str])\n",
    "row_rdd = rdd.map(lambda x: Row(x))\n",
    "schema = t.StructType().add(\"a\", t.StringType())\n",
    "\n",
    "df = spark.createDataFrame(row_rdd, schema)\n",
    "df.show()\n",
    "\n",
    "schema = t.StructType().add(\"b\", t.IntegerType())\n",
    "df_res = df.select(f.from_json(\"a\", schema).alias(\"c\"))\n",
    "df_res.printSchema()\n",
    "df_res.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+\n",
      "|a                        |\n",
      "+-------------------------+\n",
      "|{\"b\":{\"x\":1,\"y\":{\"z\":2}}}|\n",
      "+-------------------------+\n",
      "\n",
      "root\n",
      " |-- c: struct (nullable = true)\n",
      " |    |-- b: struct (nullable = true)\n",
      " |    |    |-- x: integer (nullable = true)\n",
      " |    |    |-- y: string (nullable = true)\n",
      "\n",
      "+--------------+\n",
      "|             c|\n",
      "+--------------+\n",
      "|[[1, {\"z\":2}]]|\n",
      "+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "json_str = \"\"\"{\\\"b\\\":{\\\"x\\\":1,\\\"y\\\":{\\\"z\\\":2}}}\"\"\"\n",
    "\n",
    "rdd = sc.parallelize([json_str])\n",
    "row_rdd = rdd.map(lambda x: Row(x))\n",
    "schema = t.StructType().add(\"a\", t.StringType())\n",
    "\n",
    "df = spark.createDataFrame(row_rdd, schema)\n",
    "df.show(1, False)\n",
    "\n",
    "schema = t.StructType().add(\"b\", t.StructType().add(\"x\", t.IntegerType()).add(\"y\", t.StringType()))\n",
    "df_res = df.select(f.from_json(\"a\", schema).alias(\"c\"))\n",
    "df_res.printSchema()\n",
    "df_res.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse a set of fields from a column containing JSON\n",
    "\n",
    "JSON Value 의 String 형태가 JSON 포맷이라면 json_tuple 함수를 통해 바로 파싱후 필드 값을 가져오는 것도 가능합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|a      |\n",
      "+-------+\n",
      "|{\"b\":1}|\n",
      "+-------+\n",
      "\n",
      "root\n",
      " |-- c: string (nullable = true)\n",
      "\n",
      "+---+\n",
      "|  c|\n",
      "+---+\n",
      "|  1|\n",
      "+---+\n",
      "\n",
      "root\n",
      " |-- c: string (nullable = true)\n",
      "\n",
      "+---+\n",
      "|  c|\n",
      "+---+\n",
      "|  1|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "json_str = \"\"\"{\\\"b\\\":1}\"\"\"\n",
    "\n",
    "rdd = sc.parallelize([json_str])\n",
    "row_rdd = rdd.map(lambda x: Row(x))\n",
    "schema = t.StructType().add(\"a\", t.StringType())\n",
    "\n",
    "df = spark.createDataFrame(row_rdd, schema)\n",
    "df.createOrReplaceTempView(\"view\")\n",
    "df.show(1, False)\n",
    "\n",
    "df_res = df.select(f.json_tuple(\"a\", \"b\").alias(\"c\"))\n",
    "df_res.printSchema()\n",
    "df_res.show()\n",
    "\n",
    "df_res = spark.sql('select json_tuple(a, \"b\") as c from view')\n",
    "df_res.printSchema()\n",
    "df_res.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse a well-formed string column\n",
    "\n",
    "JSON 의 String Type Value 에서 regrexp_extract 를 이용하여 원하는 문자를 추출할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- c: string (nullable = true)\n",
      "\n",
      "+---+\n",
      "|  c|\n",
      "+---+\n",
      "|  x|\n",
      "|  y|\n",
      "+---+\n",
      "\n",
      "root\n",
      " |-- c: string (nullable = true)\n",
      "\n",
      "+---+\n",
      "|  c|\n",
      "+---+\n",
      "|  x|\n",
      "|  y|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "json_str = \"\"\"\n",
    "[{ \"a\": \"x: 1\" }, { \"a\": \"y: 2\" }]\n",
    "\"\"\"\n",
    "\n",
    "rdd = sc.parallelize([json_str])\n",
    "df = spark.read.json(rdd)\n",
    "df.createOrReplaceTempView(\"view\")\n",
    "\n",
    "df_res = df.select(f.regexp_extract(\"a\", \"([a-z]):\", 1).alias(\"c\"))\n",
    "df_res.printSchema()\n",
    "df_res.show()\n",
    "\n",
    "df_res = spark.sql('select regexp_extract(a, \"([a-z]):\", 1) as c from view')\n",
    "df_res.printSchema()\n",
    "df_res.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "nikola": {
   "category": "",
   "date": "2019-05-25",
   "description": "",
   "link": "",
   "slug": "de-spark-dataframe-complex-data-format",
   "tags": "",
   "title": "Data Engineering - Apache Spark Dataframe 복잡한 데이터 포맷 다루기",
   "type": "text"
  },
  "notebookId": 3566706889179044,
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
