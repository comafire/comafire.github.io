{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Classification\n",
    "\n",
    "앙상블 기법은 모델의 예측 성능을 높이기 위해 여러 개의 기본 모델들의 결과를 조합하여 사용하는 방식입니다. 선형/비선형 데이터셋 및 분류/회귀 문제 모두에 사용할수 있습니다.\n",
    "\n",
    "* Bagging methods: 데이터를 복원 랜덤 샘플링하여 각각 독립적인 모델을 학습 시킨 후, 각 모델이 예측 한 값의 평균(Average, 연속형 변수) 또는 다수 투표(Majority Voting, 범주형 변수)방식을 이용하여 예측하는 방식\n",
    "  * 예: Random Forest, ...\n",
    "![Bagging](https://cdn-images-1.medium.com/max/1600/1*DFHUbdz6EyOuMYP4pDnFlw.jpeg)\n",
    "  \n",
    "* Boosting methods: 약한 학습기(weak learner)을 학습 시키고, 예측 오류를 최소화 하기위해 약한 학습기(weak learner)를 추가하는 식의 순차적 학습 방식을 통해 강한 학습기(strong learner)를 만드는 방법\n",
    "  * 예: AdaBoost, XGBoost, GradientBoost, ...\n",
    "\n",
    "![Boosting](https://cdn-images-1.medium.com/max/1600/1*C7CrBG1VNaa1x491eZ3fnw.gif)\n",
    "\n",
    "* Stacking methods: 모델링 알고리즘(SVM, RandomForest, XGBoost, ...)들은 각기 다른 장단점을 가지기 때문에 서로 다른 모델링 알고리즘을 조합해서 최고의 성능을 내는 모델을 생성하는 방법\n",
    "  * 예: StackNet, ...\n",
    "\n",
    "![Stacking](https://cdn-images-1.medium.com/max/1600/0*GHYCJIjkkrP5ZgPh.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data shape: np_data_xs=(10000, 10), np_data_ys=(10000,)\n",
      "train shape: np_train_xs=(7000, 10), np_train_ys=(7000,)\n",
      "test shape: np_test_xs=(3000, 10), np_test_ys=(3000,)\n",
      "\n",
      "model=BaggingClassifier(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=10, n_jobs=None, oob_score=False, random_state=None,\n",
      "         verbose=0, warm_start=False)\n",
      "acc=0.93700\n",
      "classification_report\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.91      0.92      1021\n",
      "           1       0.93      0.93      0.93      1011\n",
      "           2       0.95      0.98      0.97       968\n",
      "\n",
      "   micro avg       0.94      0.94      0.94      3000\n",
      "   macro avg       0.94      0.94      0.94      3000\n",
      "weighted avg       0.94      0.94      0.94      3000\n",
      "\n",
      "\n",
      "model=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators='warn', n_jobs=None,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False)\n",
      "acc=0.93700\n",
      "classification_report\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.92      0.92      1021\n",
      "           1       0.94      0.92      0.93      1011\n",
      "           2       0.96      0.97      0.96       968\n",
      "\n",
      "   micro avg       0.94      0.94      0.94      3000\n",
      "   macro avg       0.94      0.94      0.94      3000\n",
      "weighted avg       0.94      0.94      0.94      3000\n",
      "\n",
      "\n",
      "model=AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
      "          learning_rate=1.0, n_estimators=50, random_state=None)\n",
      "acc=0.87333\n",
      "classification_report\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.88      0.84      1021\n",
      "           1       0.91      0.80      0.85      1011\n",
      "           2       0.92      0.94      0.93       968\n",
      "\n",
      "   micro avg       0.87      0.87      0.87      3000\n",
      "   macro avg       0.88      0.87      0.87      3000\n",
      "weighted avg       0.88      0.87      0.87      3000\n",
      "\n",
      "\n",
      "model=GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
      "              learning_rate=0.1, loss='deviance', max_depth=3,\n",
      "              max_features=None, max_leaf_nodes=None,\n",
      "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "              min_samples_leaf=1, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "              n_iter_no_change=None, presort='auto', random_state=None,\n",
      "              subsample=1.0, tol=0.0001, validation_fraction=0.1,\n",
      "              verbose=0, warm_start=False)\n",
      "acc=0.93667\n",
      "classification_report\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.91      0.92      1021\n",
      "           1       0.93      0.92      0.93      1011\n",
      "           2       0.95      0.98      0.96       968\n",
      "\n",
      "   micro avg       0.94      0.94      0.94      3000\n",
      "   macro avg       0.94      0.94      0.94      3000\n",
      "weighted avg       0.94      0.94      0.94      3000\n",
      "\n",
      "\n",
      "model=XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
      "       max_depth=3, min_child_weight=1, missing=None, n_estimators=100,\n",
      "       n_jobs=1, nthread=None, objective='binary:logistic', random_state=0,\n",
      "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
      "       silent=True, subsample=1)\n",
      "acc=0.93367\n",
      "classification_report\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.91      0.92      1021\n",
      "           1       0.93      0.92      0.93      1011\n",
      "           2       0.94      0.98      0.96       968\n",
      "\n",
      "   micro avg       0.93      0.93      0.93      3000\n",
      "   macro avg       0.93      0.93      0.93      3000\n",
      "weighted avg       0.93      0.93      0.93      3000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn import datasets, model_selection, ensemble, metrics\n",
    "\n",
    "# 데이터\n",
    "np.random.seed(0)\n",
    "n_samples = 10000\n",
    "np_data_xs, np_data_ys = datasets.make_classification(\n",
    "    n_samples=n_samples, # 데이터 수\n",
    "    n_features=10, # X feature 수\n",
    "    n_informative=3,\n",
    "    n_classes=3, # Y class 수\n",
    "    random_state=0) # 난수 발생용 Seed 값\n",
    "print(\"data shape: np_data_xs={}, np_data_ys={}\".format(np_data_xs.shape, np_data_ys.shape))\n",
    "np_train_xs, np_test_xs, np_train_ys, np_test_ys = model_selection.train_test_split(\n",
    "    np_data_xs, np_data_ys, \n",
    "    test_size=0.3, shuffle=True, random_state=2)\n",
    "print(\"train shape: np_train_xs={}, np_train_ys={}\".format(np_train_xs.shape, np_train_ys.shape))\n",
    "print(\"test shape: np_test_xs={}, np_test_ys={}\".format(np_test_xs.shape, np_test_ys.shape))\n",
    "\n",
    "# 모델\n",
    "models = [\n",
    "    ensemble.BaggingClassifier(),\n",
    "    ensemble.RandomForestClassifier(),\n",
    "    ensemble.AdaBoostClassifier(),\n",
    "    ensemble.GradientBoostingClassifier(),\n",
    "    xgb.XGBClassifier()\n",
    "]\n",
    "\n",
    "for model in models:\n",
    "    # 학습\n",
    "    print(\"\\nmodel={}\".format(model))\n",
    "    model.fit(np_train_xs, np_train_ys)\n",
    "\n",
    "    # 평가\n",
    "    np_pred_ys = model.predict(np_test_xs)\n",
    "\n",
    "    acc = metrics.accuracy_score(np_test_ys, np_pred_ys)\n",
    "    print(\"acc={:.5f}\".format(acc))\n",
    "\n",
    "    cr = metrics.classification_report(np_test_ys, np_pred_ys)\n",
    "    print(\"classification_report\\n\", cr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "nikola": {
   "category": "",
   "date": "2019-04-06",
   "description": "",
   "link": "",
   "slug": "ml-ensemble-classification",
   "tags": "",
   "title": "Machine Learning - Ensemble Classification",
   "type": "text"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
