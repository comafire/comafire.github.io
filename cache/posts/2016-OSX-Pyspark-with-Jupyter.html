<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Spark">Spark<a class="anchor-link" href="#Spark">&#182;</a></h2><p>Spark 하면 먼저 클러스터를 떠올리게 되는데, 사실 로컬상에서 Spark을 이용해 프로그래밍을 하는 것도 복잡한 멀티프로세스, 멀티스레드 프로그래밍의 수고를 덜면서 로컬노드의 멀티 코어 자원을 손쉽게 활용할 수 있는 방법이기도 합니다.</p>
<p>맥북에 Spark을 설치하고 Jupyter를 통해 Python을 이용하여 손쉽게 Spark을 사용해 봅시다.</p>
<!-- TEASER_END -->

<h3 id="Spark-Install">Spark Install<a class="anchor-link" href="#Spark-Install">&#182;</a></h3><p>Java가 설치되어 있는지 확인합니다. 설치가 안되어 있다면 <a href="http://www.oracle.com/technetwork/java/javase/downloads/index.html">http://www.oracle.com/technetwork/java/javase/downloads/index.html</a> 오라클 사이트에서 다운받아 설치합니다.</p>

<pre><code>&gt; which java
/usr/bin/java
&gt; java -version
java version "1.8.0_91"
Java(TM) SE Runtime Environment (build 1.8.0_91-b14)
Java HotSpot(TM) 64-Bit Server VM (build 25.91-b14, mixed mode)</code></pre>
<p>Scala를 brew를 통해 설치합니다.</p>

<pre><code>&gt; brew install scala</code></pre>
<p>Python 을 brew를 이용해 설치하고 의존성 패키지를 설치합니다.</p>

<pre><code>&gt; brew install python
&gt; sudo easy_install pip
&gt; sudo pip install py4j
&gt; sudo pip install ipython[all]
&gt; sudo pip install jupyter</code></pre>
<p><a href="https://spark.apache.org/downloads.html">https://spark.apache.org/downloads.html</a> 사이트에서 원하는 버전의 Spark 을 다운로드 받습니다.</p>
<p>로컬에 Spark을 설치하는 것은 간단히 Spark을 받아 압축을 푸는 것으로 끝납니다. 편의를 위해서 심볼릭 링크를 걸어둡니다.</p>

<pre><code>&gt; cd /usr/local
&gt; mv ~/Downloads/spark-2.0.0-bin-hadoop2.7.tgz ./
&gt; tar -zxvf spark-2.0.0-bin-hadoop2.7.tgz
&gt; ln -s spark-2.0.0-bin-hadoop2.7 spark</code></pre>
<p>이제 쉘 환경설정파일에 환경변수를 설정합니다.</p>

<pre><code>&gt; vi ~/.bash_profile

export PATH="/usr/local/sbin:$PATH"

export JAVA_HOME=$(/usr/libexec/java_home)
export SCALA_HOME=/usr/local/bin/scala
export PATH=$PATH:$SCALA_HOME/bin

export SPARK_HOME=/usr/local/spark
export PATH=$PATH:$SPARK_HOME/bin
export PYTHONPATH=$SPARK_HOME/python/:$PYTHONPATH
export PYTHONPATH=$SPARK_HOME/python/lib/py4j-0.10-src.zip:$PYTHONPATH

export PYTHON=/usr/local/bin/python</code></pre>
<p>py4j 라이브러리의 경우 설치한 spark 안에 라이브러리 버전을 확인하여 이름을 넣어주세요.</p>
<p>이제 notebook 디렉토리로 사용할 디렉토리를 생성하고 해당 디렉토리내에서 notebook을 실행하시면 브라우저가 실행되면서 친숙한 화면을 보실수 있습니다.</p>

<pre><code>&gt; mkdir -p ~/Projects/notebooks
&gt; cd ~/Projects/notebooks
&gt; ipython notebook</code></pre>

</div>
</div>
</div>
 

