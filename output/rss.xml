<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="assets/xml/rss.xsl" media="all"?><rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Comafire's Lab</title><link>https://comafire.github.io/</link><description>Comafire's Lab</description><atom:link href="https://comafire.github.io/rss.xml" rel="self" type="application/rss+xml"></atom:link><language>en</language><copyright>Contents © 2019 &lt;a href="mailto:comafire@gmail.com"&gt;comafire&lt;/a&gt; </copyright><lastBuildDate>Tue, 20 Aug 2019 15:38:47 GMT</lastBuildDate><generator>Nikola (getnikola.com)</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>iconv로 파일 인코딩 변환</title><link>https://comafire.github.io/posts/2019-iconv/</link><dc:creator>comafire</dc:creator><description>&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;현재는 UTF-8 이 표준이 되었지만, 가끔 오래된 서버들은 아직 EUC-KR을 기본 인코딩으로 사용하는 경우가 많이 있습니다.&lt;/p&gt;
&lt;p&gt;이 경우 EUC-KR 또는 CP949 등의 예전 한글 인코딩 방식을 가지고 있는 서버에서 생성된 파일을 UTF-8 인코딩 기반 시스템에서 볼 경우 파일내 한글이 깨져 보이게 됩니다.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://comafire.github.io/posts/2019-iconv/"&gt;Read more…&lt;/a&gt; (1 min remaining to read)&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><guid>https://comafire.github.io/posts/2019-iconv/</guid><pubDate>Fri, 16 Aug 2019 15:00:00 GMT</pubDate></item><item><title>Jupyter Notebook을 Shell 에서 수행하기</title><link>https://comafire.github.io/posts/2019-jupyter-nbconvert-execute/</link><dc:creator>comafire</dc:creator><description>&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;h2 id="Jupyter-에서-Notebook-을-Shell-상에서-수행하기"&gt;Jupyter 에서 Notebook 을 Shell 상에서 수행하기&lt;a class="anchor-link" href="https://comafire.github.io/posts/2019-jupyter-nbconvert-execute/#Jupyter-%EC%97%90%EC%84%9C-Notebook-%EC%9D%84-Shell-%EC%83%81%EC%97%90%EC%84%9C-%EC%88%98%ED%96%89%ED%95%98%EA%B8%B0"&gt;¶&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;Jupyter를 서버에 설치해 놓고 사용하다보면 트레이닝 중 일때 갑자기 자리를 옮겨야 할 때가 생기면 중간에 멈추고 다시 트레이닝 해야 할때가 종종 있습니다. 이럴때는 Jupyter의 Terminal 상에서 바로 nbconvert를 통해서 Notebook을 끊김 없이 수행할 수 있습니다.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://comafire.github.io/posts/2019-jupyter-nbconvert-execute/"&gt;Read more…&lt;/a&gt; (1 min remaining to read)&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><guid>https://comafire.github.io/posts/2019-jupyter-nbconvert-execute/</guid><pubDate>Fri, 15 Mar 2019 15:00:00 GMT</pubDate></item><item><title>Jupyter 노트북 셀을 API를 통해 반복적으로 지우기</title><link>https://comafire.github.io/posts/2019-jupyter-cell-clear-outputs/</link><dc:creator>comafire</dc:creator><description>&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;h2 id="clear_output-함수"&gt;clear_output 함수&lt;a class="anchor-link" href="https://comafire.github.io/posts/2019-jupyter-cell-clear-outputs/#clear_output-%ED%95%A8%EC%88%98"&gt;¶&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;Jupyter에서 프로그래밍을 할 때, 루프문을 통해서 지속 적으로 로그를 출력해야 할때가 있습니다. 루프 횟수가 작다면 그런대로 봐줄만 하지만, 조금만 횟수가 늘어나더라도 스크롤의 압박을 피할수는 없게 됩니다. 이럴때는 위해 노트북에서는 셀 출력창을 지워주는 함수가 있습니다.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://comafire.github.io/posts/2019-jupyter-cell-clear-outputs/"&gt;Read more…&lt;/a&gt; (1 min remaining to read)&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><guid>https://comafire.github.io/posts/2019-jupyter-cell-clear-outputs/</guid><pubDate>Fri, 08 Mar 2019 15:00:00 GMT</pubDate></item><item><title>데이터셋 관리자 Quilt</title><link>https://comafire.github.io/posts/2019-quilt-dataset-manager/</link><dc:creator>comafire</dc:creator><description>&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;h2 id="Quilt-Dataset-Manager"&gt;Quilt Dataset Manager&lt;a class="anchor-link" href="https://comafire.github.io/posts/2019-quilt-dataset-manager/#Quilt-Dataset-Manager"&gt;¶&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;Quilt는 데이터 사이언스 프로젝트 수행시 Python Code를 이용하여 편리하게 데이터 셋을 체계적으로 관리하고 공유할 수 있도록 만들어진 도구 입니다. 게다가 데이터 셋을 공개 패키지로 만들면 무료로 사용가능합니다.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://quiltdata.com/"&gt;https://quiltdata.com/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;홈페이지 상에는 빠르고, 재사용 가능하다는 등 다양한 장점이 있다고 설명하지만, 제일 큰 장점은 누군가 데이터 셋을 공개 패키지화해서 Quilt에 올려 놓았다면, 일일이 웹을 통해 찾아서 다운로드하는 복잡한 코드를 작성할 필요없이 간편하게 사용이 가능하다는 점입니다.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://comafire.github.io/posts/2019-quilt-dataset-manager/"&gt;Read more…&lt;/a&gt; (1 min remaining to read)&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><guid>https://comafire.github.io/posts/2019-quilt-dataset-manager/</guid><pubDate>Fri, 15 Feb 2019 15:00:00 GMT</pubDate></item><item><title>Let's Encrypt SSL 인증서 Azure 에 적용하기</title><link>https://comafire.github.io/posts/2018-convert-letsencrypt-pem-to-pfx/</link><dc:creator>comafire</dc:creator><description>&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;웹 서버 브라우저와의 안전한 통신을 위해 HTTPS를 사용하기 위해서는 SSL 인증서가 필요합니다.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://letsencrypt.org"&gt;https://letsencrypt.org&lt;/a&gt; 을 이용하면, 개인용이나, 개발용으로 싱글 DNS에 대해서 SSL 인증서를 발급받는 것을 무료로 진행할 수 있습니다.&lt;/p&gt;
&lt;p&gt;Let's encrypt에서 제공하는 퉁을 이용해 SSL 인증서를 발급 받아보고, pem 확장자의 인증 파일을 SSL 오프로드를 위해 사용한 azure application gateway에서 사용할 pfx 확장자 인증 파일로 변환해 봅니다.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://comafire.github.io/posts/2018-convert-letsencrypt-pem-to-pfx/"&gt;Read more…&lt;/a&gt; (1 min remaining to read)&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><guid>https://comafire.github.io/posts/2018-convert-letsencrypt-pem-to-pfx/</guid><pubDate>Sat, 21 Jul 2018 15:00:00 GMT</pubDate></item><item><title>SKP에서 Mnist 데이터 분석해보기</title><link>https://comafire.github.io/posts/2018-skp-mnist/</link><dc:creator>comafire</dc:creator><description>&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;h2 id="README"&gt;README&lt;a class="anchor-link" href="https://comafire.github.io/posts/2018-skp-mnist/#README"&gt;¶&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;MNIST 데이터 셋을 우편 번호 손글씨 인식이라는 가상 프로젝트의 데이터 셋으로 가정하여 SKP 상에서 간단한 Data Science 프로젝트를 진행해 봅니다.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;notebooks-skp/mnist: &lt;a href="https://github.com/comafire/notebooks-skp/tree/master/mnist"&gt;https://github.com/comafire/notebooks-skp/tree/master/mnist&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;이 프로젝트를 통해서 볼 수 있는 내용은 아래와 같습니다.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Airflow를 통한 Data Pipeline 수행 스켸쥴링&lt;/li&gt;
&lt;li&gt;Jupyter를 통한 Data Pipeline Task 개발&lt;/li&gt;
&lt;li&gt;Keras(Tensorflow backend)를 이용한 Deep Neural Network, Convolution Neural Network 모델링&lt;/li&gt;
&lt;li&gt;Scikit Learn을 이용한 ETL(Extract Load Transform), EDA(Exploratory Data Analysis), Baseline 모델링&lt;/li&gt;
&lt;li&gt;Flask를 이용한 Backend REST API 개발&lt;/li&gt;
&lt;li&gt;Nginx, VueJS, HTML, CSS를 이용한 Front Web UI 개발&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;이제 Data Science Methodology 에 의해 프로젝트를 진행해봅니다.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://comafire.github.io/posts/2018-skp-mnist/"&gt;Read more…&lt;/a&gt; (3 min remaining to read)&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><guid>https://comafire.github.io/posts/2018-skp-mnist/</guid><pubDate>Sat, 21 Jul 2018 15:00:00 GMT</pubDate></item><item><title>SKP상에서 데이터 프로덕트 개발</title><link>https://comafire.github.io/posts/2018-notebooks-skp/</link><dc:creator>comafire</dc:creator><description>&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;h2 id="Data-Product-Projects"&gt;Data Product Projects&lt;a class="anchor-link" href="https://comafire.github.io/posts/2018-notebooks-skp/#Data-Product-Projects"&gt;¶&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;notebooks-skp 는 St. Kilda Pier 상에서 오픈된 데이터 셋을 이용하여 다양한 데이터 사이언스 프로젝트를 진행할 예정입니다.&lt;/p&gt;
&lt;p&gt;SKP 위에서 Data Collection 부터 Data Product 빌드까지 진행해 보면서, 데이터 사이언스 전반의 스킬들을 이해하고 적용하는 역활을 수행할 것입니다.&lt;/p&gt;
&lt;p&gt;St. Kilda Pier 는 빅테이더를 위한 Docker 기반 데이터 사이언스 플랫폼 입니다.&lt;/p&gt;
&lt;p&gt;전체 소스는 아래 Github 에서 확인할 수 있습니다.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;St.Kilda Pier: &lt;a href="https://github.com/comafire/st-kilda-pier"&gt;https://github.com/comafire/st-kilda-pier&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;notebooks-skp: &lt;a href="https://github.com/comafire/notebooks-skp"&gt;https://github.com/comafire/notebooks-skp&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;처음으로, MNIST 데이터 셋을 이용하는 가상의 프로젝트를 통한 템플릿 성격의 프로젝트가 올라와 있으며, 다양한 데이터 사이언스 프로젝트들이 추가될 예정입니다.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;mnist: &lt;a href="https://github.com/comafire/notebooks-skp/tree/master/mnist"&gt;https://github.com/comafire/notebooks-skp/tree/master/mnist&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a href="https://comafire.github.io/posts/2018-notebooks-skp/"&gt;Read more…&lt;/a&gt; (1 min remaining to read)&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><guid>https://comafire.github.io/posts/2018-notebooks-skp/</guid><pubDate>Sat, 14 Jul 2018 15:00:00 GMT</pubDate></item><item><title>도커 클러스터기반 빅데이터 데이터사이언스 플랫폼</title><link>https://comafire.github.io/posts/2018-st-kilda-pier/</link><dc:creator>comafire</dc:creator><description>&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;h2 id="Docker-Cluster-based-Data-Science-Platform-for-Big-Data"&gt;Docker Cluster based Data Science Platform for Big Data&lt;a class="anchor-link" href="https://comafire.github.io/posts/2018-st-kilda-pier/#Docker-Cluster-based-Data-Science-Platform-for-Big-Data"&gt;¶&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;데이터 사이언스를 하면서 그동안 필요한 많은 도구들을 설치하고 설정하는 작업을 진행해 오다가, 개인적으로 시작때 마다 하는 삽질의 양을 줄이기 위해 사용해보고 좋은 도구들은 Docker 에 넣어서 정형화 하는 작업들을 해왔습니다.&lt;/p&gt;
&lt;p&gt;많은 상용 플랫폼들이 존재하지만, 개인적인 중/소규모 데이터 사이언스 프로젝트를 진행하는데, 클라우드에 분석 클러스터를 올려놓고 쓰기에는 비용이 부담스러운 면이 없지 않아 있습니다.&lt;/p&gt;
&lt;p&gt;그래서, &lt;strong&gt;St. Kilda Pier&lt;/strong&gt;(리틀 펭귄이 서식하는 호주 멜버른의 작은 부두, 이하 SKP)라는 이름으로 데이터 사이언스 전반에 필요한 좋은 도구들을 좀 편하게 설치하고 사용할 수 있도록, 부족하지만 그동안 작업했었던 결과들을 정리해서 오픈합니다. 아직은 사용자 친화적이지 않은 명령어 기반이지만, 공감하시는 분들의 참여와 잉여시간을 지속적으로 투자하다보면 더욱 좋아지리라 생각합니다.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;st-kilda-pier: &lt;a href="https://github.com/comafire/st-kilda-pier"&gt;https://github.com/comafire/st-kilda-pier&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;notebooks-skp: &lt;a href="https://github.com/comafire/notebooks-skp"&gt;https://github.com/comafire/notebooks-skp&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a href="https://comafire.github.io/posts/2018-st-kilda-pier/"&gt;Read more…&lt;/a&gt; (1 min remaining to read)&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><guid>https://comafire.github.io/posts/2018-st-kilda-pier/</guid><pubDate>Sat, 30 Jun 2018 15:00:00 GMT</pubDate></item><item><title>Jupyter 에서 PySpark 수행시 Python 버전 충돌날때</title><link>https://comafire.github.io/posts/2018-pyspark-python-version-mismatch/</link><dc:creator>comafire</dc:creator><description>&lt;div&gt;&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;Jupyter Python3 Kernel 을 이용해서 Pyspark으로 Spark Cluster를 통해 Job 을 수행 할때, 아래와 같이 Driver 와 Worker 들간에 Python 버전 충돌이 일어 날 때가 있습니다.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Exception: Python in worker has different version 2.7 than that in driver 3.5, PySpark cannot run with different minor versions.Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;이때는 아래와 같이 두 환경 변수를 명시적으로 설정하여 해결할 수 있습니다.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;import os
os.environ["PYSPARK_PYTHON"]="/usr/bin/python3"
os.environ["PYSPARK_DRIVER_PYTHON"]="/usr/bin/python3"

spark = SparkSession.builder.master("spark://spark-skp-master:7077").appName("pysprk")
spark = spark.config("spark.cores.max", "48")
spark = spark.config("spark.driver.memory", "32g")
spark = spark.config("spark.executor.memory", "32g")
spark = spark.config("spark.python.worker.memory", "32g")
spark = spark.getOrCreate()&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In [ ]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
    &lt;div class="input_area"&gt;
&lt;div class=" highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt; 
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;&lt;/div&gt;</description><guid>https://comafire.github.io/posts/2018-pyspark-python-version-mismatch/</guid><pubDate>Fri, 25 May 2018 15:00:00 GMT</pubDate></item><item><title>BitCoin 설치</title><link>https://comafire.github.io/posts/2018-bitcoin-core-basic/</link><dc:creator>comafire</dc:creator><description>&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;Bitcoin에 대한 구조를 알아보기 위해 일단 Bitcoin을 설치해보고 간단하게 거래 테스트를 해보았습니다.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://comafire.github.io/posts/2018-bitcoin-core-basic/"&gt;Read more…&lt;/a&gt; (2 min remaining to read)&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><guid>https://comafire.github.io/posts/2018-bitcoin-core-basic/</guid><pubDate>Fri, 02 Mar 2018 15:00:00 GMT</pubDate></item></channel></rss>