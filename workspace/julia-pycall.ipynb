{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Updating cache of PyCall...\n",
      "INFO: Installing MacroTools v0.3.2\n",
      "INFO: Installing PyCall v1.7.2\n",
      "INFO: Building PyCall\n",
      "INFO: PyCall is using /usr/local/bin/python (Python 2.7.11) at /usr/local/opt/python/bin/python2.7, libpython = /usr/local/Cellar/python/2.7.11/Frameworks/Python.framework/Versions/2.7/lib/libpython2.7\n",
      "INFO: Package database updated\n",
      "INFO: METADATA is out-of-date â€” you may not have the latest version of PyCall\n",
      "INFO: Use `Pkg.update()` to get the latest versions of your packages\n"
     ]
    }
   ],
   "source": [
    "Pkg.add(\"PyCall\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Precompiling module PyCall.\n"
     ]
    }
   ],
   "source": [
    "using PyCall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "@pyimport os\n",
    "@pyimport pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PyObject <pyspark.conf.SparkConf object at 0x315058250>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf = pyspark.SparkConf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PyObject <pyspark.conf.SparkConf object at 0x315058250>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf[:setMaster](\"local[*]\")\n",
    "conf[:setAppName](\"pyspark\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16/08/19 14:41:39 INFO SparkContext: Running Spark version 1.6.0\n",
      "16/08/19 14:41:39 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/08/19 14:41:39 INFO SecurityManager: Changing view acls to: daehongseo\n",
      "16/08/19 14:41:39 INFO SecurityManager: Changing modify acls to: daehongseo\n",
      "16/08/19 14:41:39 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(daehongseo); users with modify permissions: Set(daehongseo)\n",
      "16/08/19 14:41:40 INFO Utils: Successfully started service 'sparkDriver' on port 59547.\n",
      "16/08/19 14:41:40 INFO Slf4jLogger: Slf4jLogger started\n",
      "16/08/19 14:41:40 INFO Remoting: Starting remoting\n",
      "16/08/19 14:41:40 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriverActorSystem@10.220.10.137:59554]\n",
      "16/08/19 14:41:40 INFO Utils: Successfully started service 'sparkDriverActorSystem' on port 59554.\n",
      "16/08/19 14:41:40 INFO SparkEnv: Registering MapOutputTracker\n",
      "16/08/19 14:41:40 INFO SparkEnv: Registering BlockManagerMaster\n",
      "16/08/19 14:41:40 INFO DiskBlockManager: Created local directory at /private/var/folders/0h/gc62b17x2vb0fyd8px3lbln421xhr9/T/blockmgr-4260314f-6f7c-4523-8767-e2c20ff8e596\n",
      "16/08/19 14:41:40 INFO MemoryStore: MemoryStore started with capacity 511.1 MB\n",
      "16/08/19 14:41:40 INFO SparkEnv: Registering OutputCommitCoordinator\n",
      "16/08/19 14:41:40 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
      "16/08/19 14:41:40 INFO SparkUI: Started SparkUI at http://10.220.10.137:4040\n",
      "16/08/19 14:41:40 INFO Executor: Starting executor ID driver on host localhost\n",
      "16/08/19 14:41:40 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 59560.\n",
      "16/08/19 14:41:40 INFO NettyBlockTransferService: Server created on 59560\n",
      "16/08/19 14:41:40 INFO BlockManagerMaster: Trying to register BlockManager\n",
      "16/08/19 14:41:40 INFO BlockManagerMasterEndpoint: Registering block manager localhost:59560 with 511.1 MB RAM, BlockManagerId(driver, localhost, 59560)\n",
      "16/08/19 14:41:40 INFO BlockManagerMaster: Registered BlockManager\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PyObject <pyspark.context.SparkContext object at 0x318d7fe90>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc = pyspark.SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PyObject <pyspark.sql.context.SQLContext object at 0x318dac310>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sqlContext = pyspark.sql[:SQLContext](sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16/08/19 14:54:46 INFO JSONRelation: Listing file:/usr/local/spark/examples/src/main/resources/people.json on driver\n",
      "16/08/19 14:54:46 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 209.0 KB, free 436.8 KB)\n",
      "16/08/19 14:54:46 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 19.4 KB, free 456.2 KB)\n",
      "16/08/19 14:54:46 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on localhost:59560 (size: 19.4 KB, free: 511.1 MB)\n",
      "16/08/19 14:54:46 INFO SparkContext: Created broadcast 1 from json at NativeMethodAccessorImpl.java:-2\n",
      "16/08/19 14:54:46 INFO FileInputFormat: Total input paths to process : 1\n",
      "16/08/19 14:54:46 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:-2\n",
      "16/08/19 14:54:46 INFO DAGScheduler: Got job 0 (json at NativeMethodAccessorImpl.java:-2) with 2 output partitions\n",
      "16/08/19 14:54:46 INFO DAGScheduler: Final stage: ResultStage 0 (json at NativeMethodAccessorImpl.java:-2)\n",
      "16/08/19 14:54:46 INFO DAGScheduler: Parents of final stage: List()\n",
      "16/08/19 14:54:46 INFO DAGScheduler: Missing parents: List()\n",
      "16/08/19 14:54:46 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[6] at json at NativeMethodAccessorImpl.java:-2), which has no missing parents\n",
      "16/08/19 14:54:46 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 4.3 KB, free 460.6 KB)\n",
      "16/08/19 14:54:46 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 2.4 KB, free 463.0 KB)\n",
      "16/08/19 14:54:46 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on localhost:59560 (size: 2.4 KB, free: 511.1 MB)\n",
      "16/08/19 14:54:46 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1006\n",
      "16/08/19 14:54:46 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 0 (MapPartitionsRDD[6] at json at NativeMethodAccessorImpl.java:-2)\n",
      "16/08/19 14:54:46 INFO TaskSchedulerImpl: Adding task set 0.0 with 2 tasks\n",
      "16/08/19 14:54:46 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0,PROCESS_LOCAL, 2163 bytes)\n",
      "16/08/19 14:54:46 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, partition 1,PROCESS_LOCAL, 2163 bytes)\n",
      "16/08/19 14:54:46 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)\n",
      "16/08/19 14:54:46 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)\n",
      "16/08/19 14:54:46 INFO HadoopRDD: Input split: file:/usr/local/spark/examples/src/main/resources/people.json:36+37\n",
      "16/08/19 14:54:46 INFO HadoopRDD: Input split: file:/usr/local/spark/examples/src/main/resources/people.json:0+36\n",
      "16/08/19 14:54:46 INFO deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
      "16/08/19 14:54:46 INFO deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "16/08/19 14:54:46 INFO deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
      "16/08/19 14:54:46 INFO deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
      "16/08/19 14:54:46 INFO deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
      "16/08/19 14:54:46 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2845 bytes result sent to driver\n",
      "16/08/19 14:54:46 INFO Executor: Finished task 1.0 in stage 0.0 (TID 1). 2845 bytes result sent to driver\n",
      "16/08/19 14:54:47 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 446 ms on localhost (1/2)\n",
      "16/08/19 14:54:47 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 435 ms on localhost (2/2)\n",
      "16/08/19 14:54:47 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
      "16/08/19 14:54:47 INFO DAGScheduler: ResultStage 0 (json at NativeMethodAccessorImpl.java:-2) finished in 0.456 s\n",
      "16/08/19 14:54:47 INFO DAGScheduler: Job 0 finished: json at NativeMethodAccessorImpl.java:-2, took 0.497929 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PyObject DataFrame[age: bigint, name: string]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = sqlContext[:read][:json](\"/usr/local/spark/examples/src/main/resources/people.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16/08/19 14:54:55 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 61.8 KB, free 524.8 KB)\n",
      "16/08/19 14:54:55 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 19.3 KB, free 544.1 KB)\n",
      "16/08/19 14:54:55 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on localhost:59560 (size: 19.3 KB, free: 511.1 MB)\n",
      "16/08/19 14:54:55 INFO SparkContext: Created broadcast 3 from showString at NativeMethodAccessorImpl.java:-2\n",
      "16/08/19 14:54:55 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 209.0 KB, free 753.1 KB)\n",
      "16/08/19 14:54:55 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 19.4 KB, free 772.5 KB)\n",
      "16/08/19 14:54:55 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on localhost:59560 (size: 19.4 KB, free: 511.0 MB)\n",
      "16/08/19 14:54:55 INFO SparkContext: Created broadcast 4 from showString at NativeMethodAccessorImpl.java:-2\n",
      "16/08/19 14:54:55 INFO FileInputFormat: Total input paths to process : 1\n",
      "16/08/19 14:54:55 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:-2\n",
      "16/08/19 14:54:55 INFO DAGScheduler: Got job 1 (showString at NativeMethodAccessorImpl.java:-2) with 1 output partitions\n",
      "16/08/19 14:54:55 INFO DAGScheduler: Final stage: ResultStage 1 (showString at NativeMethodAccessorImpl.java:-2)\n",
      "16/08/19 14:54:55 INFO DAGScheduler: Parents of final stage: List()\n",
      "16/08/19 14:54:55 INFO DAGScheduler: Missing parents: List()\n",
      "16/08/19 14:54:55 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[12] at showString at NativeMethodAccessorImpl.java:-2), which has no missing parents\n",
      "16/08/19 14:54:55 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 5.7 KB, free 778.2 KB)\n",
      "16/08/19 14:54:55 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 3.3 KB, free 781.5 KB)\n",
      "16/08/19 14:54:55 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on localhost:59560 (size: 3.3 KB, free: 511.0 MB)\n",
      "16/08/19 14:54:55 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1006\n",
      "16/08/19 14:54:55 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[12] at showString at NativeMethodAccessorImpl.java:-2)\n",
      "16/08/19 14:54:55 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks\n",
      "16/08/19 14:54:55 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 2, localhost, partition 0,PROCESS_LOCAL, 2163 bytes)\n",
      "16/08/19 14:54:55 INFO Executor: Running task 0.0 in stage 1.0 (TID 2)\n",
      "16/08/19 14:54:55 INFO HadoopRDD: Input split: file:/usr/local/spark/examples/src/main/resources/people.json:0+36\n",
      "16/08/19 14:54:55 INFO GenerateUnsafeProjection: Code generated in 82.406354 ms\n",
      "16/08/19 14:54:55 INFO GenerateSafeProjection: Code generated in 6.135649 ms\n",
      "16/08/19 14:54:55 INFO Executor: Finished task 0.0 in stage 1.0 (TID 2). 2459 bytes result sent to driver\n",
      "16/08/19 14:54:55 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 2) in 117 ms on localhost (1/1)\n",
      "16/08/19 14:54:55 INFO DAGScheduler: ResultStage 1 (showString at NativeMethodAccessorImpl.java:-2) finished in 0.117 s\n",
      "16/08/19 14:54:55 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
      "16/08/19 14:54:55 INFO DAGScheduler: Job 1 finished: showString at NativeMethodAccessorImpl.java:-2, took 0.124535 s\n",
      "16/08/19 14:54:55 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:-2\n",
      "16/08/19 14:54:55 INFO DAGScheduler: Got job 2 (showString at NativeMethodAccessorImpl.java:-2) with 1 output partitions\n",
      "16/08/19 14:54:55 INFO DAGScheduler: Final stage: ResultStage 2 (showString at NativeMethodAccessorImpl.java:-2)\n",
      "16/08/19 14:54:55 INFO DAGScheduler: Parents of final stage: List()\n",
      "16/08/19 14:54:55 INFO DAGScheduler: Missing parents: List()\n",
      "16/08/19 14:54:55 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[12] at showString at NativeMethodAccessorImpl.java:-2), which has no missing parents\n",
      "16/08/19 14:54:55 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 5.7 KB, free 787.1 KB)\n",
      "16/08/19 14:54:55 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 3.3 KB, free 790.4 KB)\n",
      "16/08/19 14:54:55 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on localhost:59560 (size: 3.3 KB, free: 511.0 MB)\n",
      "16/08/19 14:54:55 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1006\n",
      "16/08/19 14:54:55 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[12] at showString at NativeMethodAccessorImpl.java:-2)\n",
      "16/08/19 14:54:55 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks\n",
      "16/08/19 14:54:55 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 3, localhost, partition 1,PROCESS_LOCAL, 2163 bytes)\n",
      "16/08/19 14:54:56 INFO Executor: Running task 0.0 in stage 2.0 (TID 3)\n",
      "16/08/19 14:54:56 INFO HadoopRDD: Input split: file:/usr/local/spark/examples/src/main/resources/people.json:36+37\n",
      "16/08/19 14:54:56 INFO Executor: Finished task 0.0 in stage 2.0 (TID 3). 2424 bytes result sent to driver\n",
      "16/08/19 14:54:56 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 3) in 8 ms on localhost (1/1)\n",
      "16/08/19 14:54:56 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool \n",
      "16/08/19 14:54:56 INFO DAGScheduler: ResultStage 2 (showString at NativeMethodAccessorImpl.java:-2) finished in 0.008 s\n",
      "16/08/19 14:54:56 INFO DAGScheduler: Job 2 finished: showString at NativeMethodAccessorImpl.java:-2, took 0.015798 s\n"
     ]
    }
   ],
   "source": [
    "df[:show]()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.5.0",
   "language": "julia",
   "name": "julia-0.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
